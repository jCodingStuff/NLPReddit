{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import praw and configure our reddit API to be able to retrieve submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "\n",
    "reddit = praw.Reddit(client_id='D52pMTitN_4Okg',\n",
    "                     client_secret='6sOjBsPe-EryAiLdvgoRLyFKrVA',\n",
    "                     user_agent='ChomskyBot',\n",
    "                     username='ChomskyBot',\n",
    "                     password='CaRaGio12625')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('showerthoughts')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will retrieve all posts from a specific subreddit from a specific date onwards. After 500 posts are retrieved and labelled they will be stored in a given file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def post_retrieval(file, subreddit, date, n):\n",
    "    \n",
    "    submissions = api.search_submissions(after=date, subreddit=subreddit, limit=None, sort=\"date:asc\")\n",
    "    \n",
    "    for submission in submissions:\n",
    "        #Retrieve set variables from the submission object and store them in a post dictionary\n",
    "        post = vars(submission)\n",
    "        post_dict = {field:post[field] for field in post_fields[:-2]}\n",
    "        \n",
    "        #Retrieve 10 top comments and stors them as a dict, this is used for labelling of non-flaired posts\n",
    "        comments = []\n",
    "        if n > 0:\n",
    "            for i in range(1, min(n+1, len(submission.comments.list()))):\n",
    "                comment = submission.comments.list()[i]\n",
    "                content = vars(comment)\n",
    "                comment_dict = {field:content[field] for field in comment_fields if field in content}\n",
    "                if 'body' in content:\n",
    "                    comments.append(comment_dict)\n",
    "            post_dict['comments'] = comments\n",
    "        \n",
    "        \n",
    "        posts.append(post_dict)\n",
    "        \n",
    "        #After 50 posts are added they're written into the data file, this is done to ensure that the progress\n",
    "        #is not lost in case of internet connection issues or other problems\n",
    "        if len(posts)%500 == 0:\n",
    "            print(\"We're at: \" + str(len(posts)))\n",
    "            with open(file, 'w+') as f:\n",
    "                json.dump(posts, f)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets the fields to be retrieved and the dates from which we start retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're at: 500\n",
      "We're at: 1000\n",
      "We're at: 1500\n",
      "We're at: 2000\n",
      "We're at: 2500\n",
      "We're at: 3000\n",
      "We're at: 3500\n",
      "We're at: 4000\n",
      "We're at: 4500\n",
      "We're at: 5000\n",
      "We're at: 5500\n",
      "We're at: 6000\n",
      "We're at: 6500\n",
      "We're at: 7000\n",
      "We're at: 7500\n",
      "We're at: 8000\n",
      "We're at: 8500\n",
      "We're at: 9000\n",
      "We're at: 9500\n",
      "We're at: 10000\n",
      "We're at: 10500\n",
      "We're at: 11000\n",
      "We're at: 11500\n",
      "We're at: 12000\n",
      "We're at: 12500\n",
      "We're at: 13000\n",
      "We're at: 13500\n",
      "We're at: 14000\n",
      "We're at: 14500\n",
      "We're at: 15000\n",
      "We're at: 15500\n",
      "We're at: 16000\n",
      "We're at: 16500\n",
      "We're at: 17000\n",
      "We're at: 17500\n",
      "We're at: 18000\n",
      "We're at: 18500\n",
      "We're at: 19000\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "post_dict = {}\n",
    "\n",
    "#Fields to be retrieved from submissions and comments and subreddit from which we will be extracting them\n",
    "comments = 0\n",
    "comment_fields = ('id', 'body', 'link_id', 'permalink', 'score', 'subreddit_id')\n",
    "post_fields = ('id','name', 'created_utc', 'title', 'link_flair_text', 'selftext', 'score', 'comments', 'label')\n",
    "\n",
    "#Period from which we want to retrieve the comments\n",
    "date = int(dt.datetime(2019,1,1).timestamp())\n",
    "\n",
    "post_retrieval( subreddit+ \"data.json\", subreddit, date, comments) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
