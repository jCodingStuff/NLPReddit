{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import praw and configure our reddit API to be able to retrieve submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "\n",
    "reddit = praw.Reddit(client_id='D52pMTitN_4Okg',\n",
    "                     client_secret='6sOjBsPe-EryAiLdvgoRLyFKrVA',\n",
    "                     user_agent='ChomskyBot',\n",
    "                     username='ChomskyBot',\n",
    "                     password='CaRaGio12625')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('amitheasshole')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will label AITA posts according to their comments or flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def label(p):\n",
    "    labels = ['YTA', 'NTA', 'ESH', 'NAH']\n",
    "    label_score={label:0 for label in labels}\n",
    "    \n",
    "    #If the post hasn't been \"flaired\" (assigned a label by mods) then I calculate it myself\n",
    "    if p['link_flair_text'] == None:\n",
    "        p['label'] = comment_label(p, labels, label_score)\n",
    "        \n",
    "    #If the post has been flaired with one of the labels I assign it a 100% probability, otherwise\n",
    "    #for posts with tags like META I give the post no label\n",
    "    else:\n",
    "        if p['link_flair_text'].lower() == 'asshole':\n",
    "            label_score['YTA'] = 1\n",
    "        elif p['link_flair_text'].lower() == 'not the a-hole':\n",
    "            label_score['NTA'] = 1\n",
    "        elif p['link_flair_text'].lower() == 'everyone sucks':\n",
    "            label_score['ESH'] = 1\n",
    "        elif p['link_flair_text'].lower() == 'no a-holes here':\n",
    "            label_score['NAH'] = 1\n",
    "        else:\n",
    "            label_score = None\n",
    "    \n",
    "    p['label'] = label_score\n",
    "\n",
    "    \n",
    "def comment_label(p, labels, label_score):\n",
    "    \n",
    "    score = 0\n",
    "    label = False\n",
    "    \n",
    "    #I loop through the comments and try to look for one of the keywords, if I find it I add its score to the\n",
    "    #tags score\n",
    "    for c in p['comments']:\n",
    "        words = word_tokenize(c['body'])\n",
    "        for w in words:\n",
    "            w = w.upper()\n",
    "            if w in labels:\n",
    "                label = True\n",
    "                label_score[w] += c['score']\n",
    "                score += c['score']\n",
    "    \n",
    "    for k, v in label_score.items():\n",
    "        if score != 0:\n",
    "            label_score[k] = v/score\n",
    "            \n",
    "    \n",
    "    #If no comment contained any of the keywords for a label then I give the post no label\n",
    "    if not label:\n",
    "        label_score = None\n",
    "        \n",
    "    return label_score\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will retrieve all posts from a specific subreddit from a specific date onwards. After 500 posts are retrieved and labelled they will be stored in a given file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def post_retrieval(file, subreddit, date):\n",
    "    \n",
    "    submissions = api.search_submissions(after=date, subreddit=subreddit, limit=None, sort=\"date:asc\")\n",
    "    \n",
    "    for submission in submissions:\n",
    "        #Retrieve set variables from the submission object and store them in a post dictionary\n",
    "        post = vars(submission)\n",
    "        post_dict = {field:post[field] for field in post_fields[:-2]}\n",
    "        \n",
    "        #Retrieve 10 top comments and stors them as a dict, this is used for labelling of non-flaired posts\n",
    "        comments = []\n",
    "        if post_dict['link_flair_text'] == None:\n",
    "            for i in range(1, min(11, len(submission.comments.list()))):\n",
    "                comment = submission.comments.list()[i]\n",
    "                content = vars(comment)\n",
    "                comment_dict = {field:content[field] for field in comment_fields if field in content}\n",
    "                if 'body' in content:\n",
    "                    comments.append(comment_dict)\n",
    "        \n",
    "        post_dict['comments'] = comments\n",
    "        label(post_dict)\n",
    "        posts.append(post_dict)\n",
    "        \n",
    "        #After 50 posts are added they're written into the data file, this is done to ensure that the progress\n",
    "        #is not lost in case of internet connection issues or other problems\n",
    "        if len(posts)%500 == 0:\n",
    "            print(\"We're at: \" + str(len(posts)))\n",
    "            with open(file, 'w+') as f:\n",
    "                json.dump(posts, f)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sets the fields to be retrieved and the dates from which we start retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're at: 500\n",
      "We're at: 1000\n",
      "We're at: 1500\n",
      "We're at: 2000\n",
      "We're at: 2500\n",
      "We're at: 3000\n",
      "We're at: 3500\n",
      "We're at: 4000\n",
      "We're at: 4500\n",
      "We're at: 5000\n",
      "We're at: 5500\n",
      "We're at: 6000\n",
      "We're at: 6500\n",
      "We're at: 7000\n",
      "We're at: 7500\n",
      "We're at: 8000\n",
      "We're at: 8500\n",
      "We're at: 9000\n",
      "We're at: 9500\n",
      "We're at: 10000\n",
      "We're at: 10500\n",
      "We're at: 11000\n",
      "We're at: 11500\n",
      "We're at: 12000\n",
      "We're at: 12500\n",
      "We're at: 13000\n",
      "We're at: 13500\n",
      "We're at: 14000\n",
      "We're at: 14500\n",
      "We're at: 15000\n",
      "We're at: 15500\n",
      "We're at: 16000\n",
      "We're at: 16500\n",
      "We're at: 17000\n",
      "We're at: 17500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "posts = []\n",
    "post_dict = {}\n",
    "\n",
    "#Fields to be retrieved from submissions and comments and subreddit from which we will be extracting them\n",
    "comment_fields = ('id', 'body', 'link_id', 'permalink', 'score', 'subreddit_id')\n",
    "post_fields = ('id','name', 'created_utc', 'title', 'link_flair_text', 'selftext', 'score', 'comments', 'label')\n",
    "subreddit = 'amitheasshole'\n",
    "\n",
    "#Period from which we want to retrieve the comments\n",
    "#date = int(dt.datetime(2019,1,1).timestamp())\n",
    "date = int(1548904042.0)\n",
    "\n",
    "post_retrieval(\"data2.json\", subreddit, date) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
