{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips Prediction #\n",
    "\n",
    "In this library, we will explore the data which we retrieved from *LifeProTips*, *ShittyLifeProTips*, *UnethicalLifeProTips* and *IllegalLifeProTips*.\n",
    "Afterwards, we will make some predictions to determine which subreddit a certain post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything in case we just want to run particular cells\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and split ##\n",
    "\n",
    "Let's start by loading the posts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "posts = {}\n",
    "\n",
    "# Import LifeProTips\n",
    "with open('tips/lifeprotips_dump_1.json', 'r') as lptf:\n",
    "    posts['lpt'] = json.load(lptf)\n",
    "\n",
    "# Import ShittyLifeProTips\n",
    "with open('tips/shittylifeprotips_dump_1.json', 'r') as slptf:\n",
    "    posts['slpt'] = json.load(slptf)\n",
    "    \n",
    "# Import UnethicalLifeProTips\n",
    "with open('tips/unethicallifeprotips_dump_1.json', 'r') as ulptf:\n",
    "    posts['ulpt'] = json.load(ulptf)\n",
    "\n",
    "# Import IllegalLifeProTips\n",
    "with open('tips/illegallifeprotips_dump_1.json', 'r') as ilptf:\n",
    "    posts['ilpt'] = json.load(ilptf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove any duplicate from the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit in posts:\n",
    "    obs_ids = [] # observed ids\n",
    "    unique_posts = [] # unique posts\n",
    "    for post in posts[subreddit]: # every post is a dictionary\n",
    "        if post['id'] not in obs_ids:\n",
    "            obs_ids.append(post['id'])\n",
    "            unique_posts.append(post)\n",
    "    posts[subreddit] = unique_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the amount of unique posts we retrieved per subreddit $\\rightarrow$ No duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (s, a) in posts.items():\n",
    "    print('{} posts retrieved from {}'.format(len(a), s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why there are only 10k posts from *IllegalLifeProTips* is because the community is relatively young compared to the rest of subreddits. It was founded a year ago while the other three appeared 7 years ago.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's shuffle the posts for each subreddit to perform an arbitrary and even train/validation/test division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for (s, p) in posts.items():\n",
    "    random.shuffle(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perfom a train/validation/test split as follows:\n",
    "* $4/9$ for training ($2/3$ of $2/3$)\n",
    "* $2/9$ for validation ($1/3$ of $2/3$)\n",
    "* $1/3$ for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Create and empty dictionary to store train, validation and test posts\n",
    "posts_struct = {'train':[], 'validation':[], 'test':[]}\n",
    "\n",
    "# For each subreddit, split\n",
    "for p in posts.values():\n",
    "    # p is the list of posts (each post is a dictionary)\n",
    "    # train + validation is 2/3; test is 1/3\n",
    "    split_index = int(math.ceil((2/3)*len(p)))\n",
    "    p_trainval = p[:split_index]\n",
    "    p_test = p[split_index:]\n",
    "    # Now, out of train+validation, 2/3 go to train and 1/3 to validation\n",
    "    split_index = int(math.ceil((2/3)*len(p_trainval)))\n",
    "    p_train = p_trainval[:split_index]\n",
    "    p_validation = p_trainval[split_index:]\n",
    "    \n",
    "    # Finally, add the posts to their respective parts in the posts_struct dictionary\n",
    "    posts_struct['train'].extend(p_train)\n",
    "    posts_struct['validation'].extend(p_validation)\n",
    "    posts_struct['test'].extend(p_test)\n",
    "\n",
    "# Shuffle the lists\n",
    "for (sep, p) in posts_struct.items():\n",
    "    random.shuffle(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we performed a correct split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_tot = len(posts_struct['train']) + len(posts_struct['validation']) + len(posts_struct['test'])\n",
    "print('Total number of instances: {}'.format(ins_tot))\n",
    "print('Percentage of train instances: {}'.format((len(posts_struct['train'])/ins_tot)*100))\n",
    "print('Percentage of validation instances: {}'.format((len(posts_struct['validation'])/ins_tot)*100))\n",
    "print('Percentage of test instances: {}'.format((len(posts_struct['test'])/ins_tot)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need one more step to perform. Creating `pandas` DataFrames for training, validation and testing will make data handling easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataFrame for training\n",
    "df_train = pd.DataFrame({'instance':[s['title'] for s in posts_struct['train']],\n",
    "                         'label':[s['label'] for s in posts_struct['train']]})\n",
    "\n",
    "# DataFrame for validation\n",
    "df_validation = pd.DataFrame({'instance':[s['title'] for s in posts_struct['validation']],\n",
    "                              'label':[s['label'] for s in posts_struct['validation']]})\n",
    "\n",
    "# DataFrame for testing\n",
    "df_test = pd.DataFrame({'instance':[s['title'] for s in posts_struct['test']],\n",
    "                        'label':[s['label'] for s in posts_struct['test']]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instance pre-processing ##\n",
    "\n",
    "Define a method to clean the instances<br>\n",
    "**Disclaimer:** we are **not** copying this from Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stem_words=True):\n",
    "    \n",
    "    # Empty question\n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\"\\'t'\", \" not \", text)\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" thats \", \" that is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    #you might need more\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    for c in punctuation:\n",
    "        text = re.sub(\"\\\\\" + c, \"\", text)\n",
    "        \n",
    "    # split text\n",
    "    n_text = text.split();\n",
    "    \n",
    "    # remove stopwords and subreddit keywords\n",
    "    stops = set(['the', 'to', 'now', 'should', 'just', 'very', 'too', 'again', 'in', 'out', 'on',\n",
    "                 'over', 'at', 'of', 'about'])\n",
    "    #stops = set(stopwords.words('english'))\n",
    "    sub_kwords = set(['lpt', 'ulpt', 'ilpt', 'slpt'])\n",
    "    numbers = set(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve'])\n",
    "    n_text = [word for word in n_text if word not in stops|sub_kwords|numbers if word.isalpha()]\n",
    "    \n",
    "    # Change numbers and other similar words\n",
    "    #subs = [('one', '1'), ('ii', '2'), ('two', '2'), ('iii', '3'), ('three', '3'), ('iv', '4'), ('four', '4'),\n",
    "    #        ('v', '5'), ('five', '5'), ('vi', '6'), ('six', '6'), ('vii', '7'), ('seven', '7'), ('viii', '8'),\n",
    "    #        ('eight', '8'), ('ix', '9'), ('nine', '9'), ('ten', '10'), ('eleven', '11'), ('twelve', '12')]\n",
    "    #for i, word in enumerate(n_text):\n",
    "    #    for (bad, good) in subs:\n",
    "    #        if word == bad:\n",
    "    #            n_text[i] = good\n",
    "    #            break\n",
    "    \n",
    "    # Return a list of words\n",
    "    return ' '.join(n_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['instance'] = df_train['instance'].apply(clean)\n",
    "df_validation['instance'] = df_validation['instance'].apply(clean)\n",
    "df_test['instance'] = df_test['instance'].apply(clean)\n",
    "\n",
    "# Drop empty posts\n",
    "df_train = df_train[df_train['instance'] != \"\"]\n",
    "df_train = df_train[df_train['instance'] != \" \"]\n",
    "df_validation = df_validation[df_validation['instance'] != \"\"]\n",
    "df_validation = df_validation[df_validation['instance'] != \" \"]\n",
    "df_test = df_test[df_test['instance'] != \"\"]\n",
    "df_test = df_test[df_test['instance'] != \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the ML Models ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW + Naive Bayes ###\n",
    "\n",
    "First, build the BOW representation for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate the vocabulary using all instances\n",
    "vectorizer.fit(np.concatenate((df_train['instance'].values, df_validation['instance'].values,\n",
    "                               df_test['instance'].values), axis=0))\n",
    "\n",
    "# Transform each batch separately and add labels\n",
    "# Train\n",
    "x_train = vectorizer.transform(df_train['instance'].values).toarray()\n",
    "y_train = df_train['label'].values\n",
    "# Validation\n",
    "x_validation = vectorizer.transform(df_validation['instance'].values).toarray()\n",
    "y_validation = df_validation['label'].values\n",
    "# Train+Validation\n",
    "x_tv = np.concatenate((x_train, x_validation), axis=0)\n",
    "y_tv = np.concatenate((y_train, y_validation), axis=0)\n",
    "# Test\n",
    "x_test = vectorizer.transform(df_test['instance'].values).toarray()\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform validation using `MultinomialNB` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "alphas = np.linspace(0.3,0.8,10)\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "fscores = []\n",
    "\n",
    "top_acc = 0\n",
    "top_alpha = 0\n",
    "for alpha in alphas:\n",
    "    model = MultinomialNB(alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    acc = model.score(x_validation, y_validation)\n",
    "    accs.append(acc)\n",
    "    measures = precision_recall_fscore_support(y_validation, model.predict(x_validation), average='macro')\n",
    "    precs.append(measures[0])\n",
    "    recs.append(measures[1])\n",
    "    fscores.append(measures[2])\n",
    "    if acc > top_acc:\n",
    "        top_acc = acc\n",
    "        top_alpha = alpha\n",
    "print(\"Top accuracy:\", top_acc)\n",
    "print(\"Alpha:\", top_alpha)\n",
    "\n",
    "# Retrain with whole training set and optimal alpha\n",
    "top_model = MultinomialNB(top_alpha)\n",
    "top_model.fit(x_tv, y_tv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot measures vs alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "\n",
    "# Accuracy vs alpha\n",
    "axes[0][0].plot(alphas, accs, lw=1.5)\n",
    "axes[0][0].set_xlabel(r'$\\alpha$')\n",
    "axes[0][0].set_ylabel('Accuracy')\n",
    "axes[0][0].set_title('Accuracy vs. Alpha for MultinomialNB')\n",
    "\n",
    "# Precision vs alpha\n",
    "axes[0][1].plot(alphas, precs, lw=1.5)\n",
    "axes[0][1].set_xlabel(r'$\\alpha$')\n",
    "axes[0][1].set_ylabel('Precision')\n",
    "axes[0][1].set_title('Precision vs. Alpha for MultinomialNB')\n",
    "\n",
    "# Recall vs alpha\n",
    "axes[1][0].plot(alphas, recs, lw=1.5)\n",
    "axes[1][0].set_xlabel(r'$\\alpha$')\n",
    "axes[1][0].set_ylabel('Recall')\n",
    "axes[1][0].set_title('Recall vs. Alpha for MultinomialNB')\n",
    "\n",
    "# FScore vs alpha\n",
    "axes[1][1].plot(alphas, fscores, lw=1.5)\n",
    "axes[1][1].set_xlabel(r'$\\alpha$')\n",
    "axes[1][1].set_ylabel('FScore')\n",
    "axes[1][1].set_title('FScore vs. Alpha for MultinomialNB')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MN Accuracy: %0.2f%%\" % (100 * top_model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute confusion matrix, precision, recall and F-score with micro and macro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_test, top_model.predict(x_test))\n",
    "\n",
    "measures_macro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='macro'))[:-1]\n",
    "measures_micro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='micro'))[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df = pd.DataFrame(cf, columns=['Predicted ilpt', 'Predicted lpt', 'Predicted slpt', 'Predicted ulpt'])\n",
    "cf_df.rename(index={0:'ilpt',1:'lpt',2:'slpt',3:'ulpt'}, inplace=True)\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show precision, recall and F-score with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_macro = ['macro'] + measures_macro\n",
    "measures_micro = ['micro'] + measures_micro\n",
    "measures_df = pd.DataFrame([measures_macro, measures_micro], columns=['Averaging', 'Precision', 'Recall', 'FScore'])\n",
    "measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec ###\n",
    "\n",
    "We will train a `Word2Vec` model from `gensim` using our entire corpus.\n",
    "\n",
    "First, we define a function to tokenize a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_arr(sentences):\n",
    "    \"\"\"Given a list of sentences, return the list with the tokenized strings (a list itself).\"\"\"\n",
    "    new_sentences = sentences.copy()\n",
    "    for i, sentence in enumerate(new_sentences):\n",
    "        new_sentences[i] = sentence.split()\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the `sentences` for `Word2Vec` using all corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize_arr(np.concatenate((df_train['instance'].values,\n",
    "                                         df_validation['instance'].values,\n",
    "                                         df_test['instance'].values), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `Word2Vec` using `sentences`.<br>\n",
    "We must look for good parameter values regarding `min_count`, `size` and `window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences, min_count=1, size=10, window=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings must be generated for the instances in `df_train`, `df_validation` and `df_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the submissions in train, validation and test\n",
    "sub_train = tokenize_arr(df_train['instance'].values)\n",
    "sub_validation = tokenize_arr(df_validation['instance'].values)\n",
    "sub_test = tokenize_arr(df_test['instance'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a method to transform a list of tokens into an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_lists(lol):\n",
    "    \"\"\"Given a list of lists of equal size, sum its elements (like vector sum) and take the average.\"\"\"\n",
    "    result = []\n",
    "    for element in lol[0]: # Create the size of sum array\n",
    "        result.append(0)\n",
    "    for l in lol: # For every sublist\n",
    "        for i, c in enumerate(l): # For every element\n",
    "            result[i] += c\n",
    "    # Average the sum\n",
    "    for i, v in enumerate(result):\n",
    "        result[i] /= len(lol);\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2emb(sentence, word_dict):\n",
    "    \"\"\"Transform a tokenized sentence into an embedding by adding the embedding word vectors.\"\"\"\n",
    "    embeddings = []\n",
    "    for token in sentence:\n",
    "        if token in word_dict.vocab:\n",
    "            embeddings.append(word_dict[token].tolist())\n",
    "    embedding = sum_lists(embeddings)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $x$ for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sen2emb(sub, w2v_model.wv) for sub in sub_train]\n",
    "x_validation = [sen2emb(sub, w2v_model.wv) for sub in sub_validation]\n",
    "x_tv = np.concatenate((x_train, x_validation), axis=0)\n",
    "x_test = [sen2emb(sub, w2v_model.wv) for sub in sub_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the labels for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_validation = df_validation['label'].values\n",
    "y_tv = np.concatenate((y_train, y_validation), axis=0)\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **feature standarization** so that the values of each feature in the data have zero-mean and unit-variance.\n",
    "\n",
    "$$x'=\\frac{x-\\overline{x}}{\\sigma}$$\n",
    "\n",
    "Where $x$ is the original feature vector, $\\overline{x} = average(x)$ is the mean of that feature vector, $\\sigma$ is its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize(lol):\n",
    "    \"\"\"Given a list of list, standarize the sublists.\"\"\"\n",
    "    lol_np = np.array(lol)\n",
    "    avg_v = np.mean(lol_np, axis=0)\n",
    "    std_v = np.std(lol_np, axis=0)\n",
    "    return (lol_np - avg_v) / std_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarize $x$ for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = standarize(x_train)\n",
    "x_validation = standarize(x_validation)\n",
    "x_tv = standarize(x_tv)\n",
    "x_test = standarize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters and retrieve measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "top_accuracy = 0;\n",
    "top_reg = 0;\n",
    "\n",
    "regs = np.linspace(1e-10, 1, 10)\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "fscores = []\n",
    "\n",
    "for reg in regs:\n",
    "    svm_model = SVC(gamma='scale', C=reg)\n",
    "    svm_model.fit(x_train, y_train)\n",
    "    acc = svm_model.score(x_validation, y_validation)\n",
    "    accs.append(acc)\n",
    "    measures = precision_recall_fscore_support(y_validation, svm_model.predict(x_validation), average='macro')\n",
    "    precs.append(measures[0])\n",
    "    recs.append(measures[1])\n",
    "    fscores.append(measures[2])\n",
    "    if acc > top_accuracy:\n",
    "        top_accuracy = acc\n",
    "        top_reg = reg\n",
    "\n",
    "print(\"Top regularization:\", top_reg)\n",
    "print(\"Top accuracy:\", top_accuracy)\n",
    "\n",
    "# Retrain with whole training set and optimal penalty\n",
    "top_model = SVC(gamma='scale', C=top_reg)\n",
    "top_model.fit(x_tv, y_tv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot measures vs. penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "\n",
    "# Accuracy vs regularization\n",
    "axes[0][0].plot(regs, accs, lw=1.5)\n",
    "axes[0][0].set_xlabel('Regularization')\n",
    "axes[0][0].set_ylabel('Accuracy')\n",
    "axes[0][0].set_title('Accuracy vs. Regularization for SVM')\n",
    "\n",
    "# Precision vs regularization\n",
    "axes[0][1].plot(regs, precs, lw=1.5)\n",
    "axes[0][1].set_xlabel('Regularization')\n",
    "axes[0][1].set_ylabel('Precision')\n",
    "axes[0][1].set_title('Precision vs. Regularization for SVM')\n",
    "\n",
    "# Recall vs regularization\n",
    "axes[1][0].plot(regs, recs, lw=1.5)\n",
    "axes[1][0].set_xlabel('Regularization')\n",
    "axes[1][0].set_ylabel('Recall')\n",
    "axes[1][0].set_title('Recall vs. Regularization for SVM')\n",
    "\n",
    "# FScore vs regularization\n",
    "axes[1][1].plot(regs, fscores, lw=1.5)\n",
    "axes[1][1].set_xlabel('Regularization')\n",
    "axes[1][1].set_ylabel('FScore')\n",
    "axes[1][1].set_title('FScore vs. Regularization for SVM')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Accuracy: %0.2f%%\" % (100 * top_model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute confusion matrix, precision, recall and F-score with micro and macro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_test, top_model.predict(x_test))\n",
    "\n",
    "measures_macro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='macro'))[:-1]\n",
    "measures_micro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='micro'))[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf_df = pd.DataFrame(cf, columns=['Predicted ilpt', 'Predicted lpt', 'Predicted slpt', 'Predicted ulpt'])\n",
    "cf_df.rename(index={0:'ilpt',1:'lpt',2:'slpt',3:'ulpt'}, inplace=True)\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show precision, recall and F-score with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measures_macro = ['macro'] + measures_macro\n",
    "measures_micro = ['micro'] + measures_micro\n",
    "measures_df = pd.DataFrame([measures_macro, measures_micro], columns=['Averaging', 'Precision', 'Recall', 'FScore'])\n",
    "measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters and retrieve measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "top_accuracy = 0;\n",
    "top_reg = 0;\n",
    "\n",
    "regs = np.linspace(1e-10, 0.05, 200)\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "fscores = []\n",
    "\n",
    "for reg in regs:\n",
    "    lr_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=reg, max_iter=5000)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    acc = lr_model.score(x_validation, y_validation)\n",
    "    accs.append(acc)\n",
    "    measures = precision_recall_fscore_support(y_validation, lr_model.predict(x_validation), average='macro')\n",
    "    precs.append(measures[0])\n",
    "    recs.append(measures[1])\n",
    "    fscores.append(measures[2])\n",
    "    if acc > top_accuracy:\n",
    "        top_accuracy = acc\n",
    "        top_reg = reg\n",
    "\n",
    "print(\"Top regularization:\", top_reg)\n",
    "print(\"Top accuracy:\", top_accuracy)\n",
    "\n",
    "# Retrain with whole training set and optimal penalty\n",
    "top_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=top_reg, max_iter=10000)\n",
    "top_model.fit(x_tv, y_tv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot measures vs. penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "\n",
    "# Accuracy vs regularization\n",
    "axes[0][0].plot(regs, accs, lw=1.5)\n",
    "axes[0][0].set_xlabel('Regularization')\n",
    "axes[0][0].set_ylabel('Accuracy')\n",
    "axes[0][0].set_title('Accuracy vs. Regularization for LR')\n",
    "\n",
    "# Precision vs regularization\n",
    "axes[0][1].plot(regs, precs, lw=1.5)\n",
    "axes[0][1].set_xlabel('Regularization')\n",
    "axes[0][1].set_ylabel('Precision')\n",
    "axes[0][1].set_title('Precision vs. Regularization for LR')\n",
    "\n",
    "# Recall vs penalty\n",
    "axes[1][0].plot(regs, recs, lw=1.5)\n",
    "axes[1][0].set_xlabel('Regularization')\n",
    "axes[1][0].set_ylabel('Recall')\n",
    "axes[1][0].set_title('Recall vs. Regularization for LR')\n",
    "\n",
    "# FScore vs regularization\n",
    "axes[1][1].plot(regs, fscores, lw=1.5)\n",
    "axes[1][1].set_xlabel('Regularization')\n",
    "axes[1][1].set_ylabel('FScore')\n",
    "axes[1][1].set_title('FScore vs. Regularization for LR')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR Accuracy: %0.2f%%\" % (100 * top_model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute confusion matrix, precision, recall and F-score with micro and macro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_test, top_model.predict(x_test))\n",
    "\n",
    "measures_macro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='macro'))[:-1]\n",
    "measures_micro = list(precision_recall_fscore_support(y_test, top_model.predict(x_test), average='micro'))[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf_df = pd.DataFrame(cf, columns=['Predicted ilpt', 'Predicted lpt', 'Predicted slpt', 'Predicted ulpt'])\n",
    "cf_df.rename(index={0:'ilpt',1:'lpt',2:'slpt',3:'ulpt'}, inplace=True)\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show precision, recall and F-score with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measures_macro = ['macro'] + measures_macro\n",
    "measures_micro = ['micro'] + measures_micro\n",
    "measures_df = pd.DataFrame([measures_macro, measures_micro], columns=['Averaging', 'Precision', 'Recall', 'FScore'])\n",
    "measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word2Vec ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
